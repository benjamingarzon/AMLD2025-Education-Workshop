{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b8d34927",
      "metadata": {
        "id": "b8d34927"
      },
      "source": [
        "## Deep Knowledge Tracing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c87c5e",
      "metadata": {},
      "source": [
        "We will try out three different deep learning models. The code is in the folder DeepModels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a507e09b",
      "metadata": {},
      "source": [
        "# Data\n",
        "\n",
        "We will fit data from the Assistments platform (https://www.commonsense.org/education/website/assistments). These data are from the course 2015.\n",
        "\n",
        "First make sure to download the data from https://drive.google.com/file/d/0B_hO8cnpcIMgUGZzRnh3bHJrSjQ/view?resourcekey=0-dGtan-IMFc3IjQ749-FgQA to the folder **./DeepModels/datasets/ASSIST2015/**. The data preparation script assumes that the data are in a file **./DeepModels/datasets/ASSIST2015/2015_100_skill_builders_main_problems.csv**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c9d5f31a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2163acd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.path.abspath(os.getcwd())\n",
        "os.makedirs(\"DeepModels/datasets/ASSIST2015\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3441bede",
      "metadata": {},
      "source": [
        "Inspect the data:\n",
        "- *user_id* - Student ID. \n",
        "- *log_id* - Indicates the order in which it was presented.\n",
        "- *sequence_id* - Knowledge Component (KC). Groups items (exercises/questions) of a similar topic or skill.\n",
        "- *correct* - Whether the response is correct (1) or incorrect (0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94240d13",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = \"DeepModels/datasets/ASSIST2015\"\n",
        "data = pd.read_csv(\n",
        "    os.path.join(data_dir, \"2015_100_skill_builders_main_problems.csv\"), sep=\",\"\n",
        ")\n",
        "\n",
        "print(data.head(10))\n",
        "print(data.user_id.nunique())\n",
        "print(data.shape)\n",
        "print(data.correct.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27aa7030",
      "metadata": {},
      "source": [
        "# Preprocessing\n",
        "\n",
        "Since datasets can differ in how they are formatted. In the **data_loaders** folder there are preprocessing scripts. These are called when you run the model (no need to call them separately). We can run *train.py* with the option *--only_preprocess* to run only the preprocessing, to avoid a long wait. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d098e90d",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(\"DeepModels/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6c129767",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train.py --model_name=dkt --dataset_name=ASSIST2015 --only_preprocess "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaff23eb",
      "metadata": {},
      "source": [
        "Briefly, the preprocessing implements the following steps:\n",
        "\n",
        "- Sort responses by log_id (time) for each student.\n",
        "- Remove observations if correct is 1 or 0.\n",
        "- Reindex students and items.\n",
        "- Split sequences of KCs and responses so that they have uniform length (*seq_len*).\n",
        "- Split dataset in train and test (e.g., 90%/10%).\n",
        "\n",
        "The preprocessing creates the following files that are used to train the model:\n",
        "\n",
        "*u_list.pkl*: List of student ids.\n",
        "*u2idx.pkl*: Dict with mapping of student ids to indices.\n",
        "\n",
        "*q_list.pkl*: List of knowledge component (KC) ids.\n",
        "*q2idx.pkl*: Dict with mapping of KC ids to indices.\n",
        "\n",
        "*r_seqs.pkl*: List of sequences of responses. Each element is an array that corresponds to the sequence of responses of one student, ordered by timestamp (log_id).\n",
        "*q_seqs.pkl*: List of sequences of KCs. Each element is an array that corresponds to the sequence of KCs of one student, ordered by timestamp (log_id). It matches r_seqs.\n",
        "\n",
        "*train_indices.pkl*, *test_indices.pkl*: training/test indices\n",
        "\n",
        "Inspect these files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5313dc76",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = \"./datasets/ASSIST2015\"\n",
        "pkl_files = [f for f in os.listdir(data_dir) if f.endswith(\".pkl\")]\n",
        "\n",
        "\n",
        "data_dict = {}\n",
        "\n",
        "\n",
        "for file in pkl_files:\n",
        "\n",
        "    with open(os.path.join(data_dir, file), \"rb\") as f:\n",
        "        data_dict[os.path.splitext(file)[0]] = pickle.load(f)\n",
        "\n",
        "\n",
        "print(data_dict[\"u_list\"][:5])\n",
        "print(data_dict[\"q_list\"][:5])\n",
        "\n",
        "\n",
        "print(data_dict[\"q_seqs\"][:5])\n",
        "\n",
        "\n",
        "print(data_dict[\"r_seqs\"][:5])\n",
        "\n",
        "print(len(data_dict[\"train_indices\"]))\n",
        "print(len(data_dict[\"test_indices\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b9095f",
      "metadata": {},
      "source": [
        "*q_seqs*, *r_seqs* are finally converted in datasets which consists of pairs *(q, r)*, where *q* and *r* are 1-d arrays. *q* is a sequence of KC indices and *r* a sequence of responses (0 or 1). The sequences have length *seq_len* and are padded at the end with -1. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23898c33",
      "metadata": {},
      "source": [
        "# Configuration\n",
        "\n",
        "The configuration file config.json allows to specify parameters for the training and for the different models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e507e102",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"config.json\", \"r\") as f:\n",
        "    print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6038d1",
      "metadata": {},
      "source": [
        "# Fitting the model\n",
        "\n",
        "Now you can run the model. After running it, results (loss and aucs) for the test set can be found in the folder *ckpts*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c49e2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_name can be dkt, dkvmn or sakt\n",
        "!python train.py --model_name=dkvmn --dataset_name=ASSIST2015 "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0aa364c",
      "metadata": {},
      "source": [
        "Since this takes a while to run, you have the precomputed results for three models (dkt, dkvmn, sakt) in the folder *ckpts_precomputed/<model_name>/<dataset_name>*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce72a67f",
      "metadata": {},
      "outputs": [],
      "source": [
        "results_dict = {}\n",
        "combined_df = None\n",
        "for model_name in [\"dkt\", \"dkvmn\", \"sakt\"]:\n",
        "    ckpt_dir = f\"ckpts_precomputed/{model_name}/ASSIST2015\"\n",
        "    pkl_files = [f for f in os.listdir(ckpt_dir) if f.endswith(\".pkl\")]\n",
        "\n",
        "    if len(pkl_files) == 0:\n",
        "        continue\n",
        "\n",
        "    results_dict[model_name] = {}\n",
        "\n",
        "    for file in pkl_files:\n",
        "        with open(os.path.join(ckpt_dir, file), \"rb\") as f:\n",
        "            results_dict[model_name][os.path.splitext(file)[0]] = pickle.load(f)\n",
        "\n",
        "    # Create DataFrames for plotting\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"Epoch\": range(len(results_dict[model_name][\"aucs\"])),\n",
        "            \"AUC\": results_dict[model_name][\"aucs\"],\n",
        "            \"Loss\": results_dict[model_name][\"loss_means\"],\n",
        "            \"Model\": model_name,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if combined_df is None:\n",
        "        combined_df = df\n",
        "    else:\n",
        "        combined_df = pd.concat([combined_df, df])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "# Plot Loss\n",
        "sns.lineplot(ax=axes[0], data=combined_df, x=\"Iteration\", y=\"Loss\", hue=\"Model\")\n",
        "axes[0].set_title(\"Loss\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot AUC\n",
        "sns.lineplot(ax=axes[1], data=combined_df, x=\"Iteration\", y=\"AUC\", hue=\"Model\")\n",
        "axes[1].set_title(\"AUC\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"AUC\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AMLDAIEd",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
