{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b8d34927",
      "metadata": {
        "id": "b8d34927"
      },
      "source": [
        "## Deep Knowledge Tracing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c87c5e",
      "metadata": {},
      "source": [
        "We will try out three different deep learning models. The code is in the folder DeepModels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a507e09b",
      "metadata": {},
      "source": [
        "# Data\n",
        "\n",
        "We will fit data from the Assistments platform (https://www.commonsense.org/education/website/assistments). These data are from the course 2009.\n",
        "\n",
        "First make sure to download the data from https://drive.google.com/file/d/1AWbYGz84WHpC6UbO6PHBkBnPKpSTJPEZ/view?usp=sharing to the folder **./DeepModels/datasets/ASSIST2009/**. The data preparation script assumes that the data are in a file **./DeepModels/datasets/ASSIST2009/skill_builder_data.csv**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c9d5f31a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4303fab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# uncomment if running on colab\n",
        "# !git clone https://github.com/benjamingarzon/AMLD2025-Education-Workshop.git\n",
        "# %cd AMLD2025-Education-Workshop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2163acd0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'g:\\\\Meine Ablage\\\\Supervision\\\\AMLD-workshop\\\\AMLD2024-Education-Workshop\\\\DeepModels'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.makedirs(\"DeepModels/datasets/ASSIST2009\", exist_ok=True)\n",
        "os.path.abspath(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3441bede",
      "metadata": {},
      "source": [
        "Inspect the data:\n",
        "- *user_id* - Student ID. \n",
        "- *log_id* - Indicates the order in which it was presented.\n",
        "- *sequence_id* - Knowledge Component (KC). Groups items (exercises/questions) of a similar topic or skill.\n",
        "- *correct* - Whether the response is correct (1) or incorrect (0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "94240d13",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  order_id  assignment_id  user_id  assistment_id  problem_id  \\\n",
            "0           1  33022537         277618    64525          33139       51424   \n",
            "1           2  33022709         277618    64525          33150       51435   \n",
            "2           3  35450204         220674    70363          33159       51444   \n",
            "3           4  35450295         220674    70363          33110       51395   \n",
            "4           5  35450311         220674    70363          33196       51481   \n",
            "5           6  35450555         220674    70363          33172       51457   \n",
            "6           7  35450573         220674    70363          33174       51459   \n",
            "7           8  35480603         220674    70363          33123       51408   \n",
            "8           9  33140811         220674    70677          33168       51453   \n",
            "9          10  33140919         220674    70677          33112       51397   \n",
            "\n",
            "   original  correct  attempt_count  ms_first_response  ... hint_count  \\\n",
            "0         1        1              1              32454  ...          0   \n",
            "1         1        1              1               4922  ...          0   \n",
            "2         1        0              2              25390  ...          0   \n",
            "3         1        1              1               4859  ...          0   \n",
            "4         1        0             14              19813  ...          3   \n",
            "5         1        1              1              16031  ...          0   \n",
            "6         1        1              1              15047  ...          0   \n",
            "7         1        1              1              10732  ...          0   \n",
            "8         1        1              1              23241  ...          0   \n",
            "9         1        1              1              11512  ...          0   \n",
            "\n",
            "  hint_total  overlap_time  template_id  answer_id answer_text  first_action  \\\n",
            "0          3         32454        30799        NaN          26             0   \n",
            "1          3          4922        30799        NaN          55             0   \n",
            "2          3         42000        30799        NaN          88             0   \n",
            "3          3          4859        30059        NaN          41             0   \n",
            "4          4        124564        30060        NaN          65             0   \n",
            "5          4         16031        30060        NaN          12             0   \n",
            "6          4         15047        30060        NaN           6             0   \n",
            "7          3         10732        30059        NaN          55             0   \n",
            "8          4         23241        30060        NaN          12             0   \n",
            "9          2         11512        30059        NaN          36             0   \n",
            "\n",
            "  bottom_hint  opportunity  opportunity_original  \n",
            "0         NaN            1                   1.0  \n",
            "1         NaN            2                   2.0  \n",
            "2         NaN            1                   1.0  \n",
            "3         NaN            2                   2.0  \n",
            "4         0.0            3                   3.0  \n",
            "5         NaN            4                   4.0  \n",
            "6         NaN            5                   5.0  \n",
            "7         NaN            6                   6.0  \n",
            "8         NaN            1                   1.0  \n",
            "9         NaN            2                   2.0  \n",
            "\n",
            "[10 rows x 30 columns]\n",
            "4151\n",
            "(274590, 30)\n",
            "correct\n",
            "1    181695\n",
            "0     92895\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"DeepModels/datasets/ASSIST2009\"\n",
        "data = pd.read_csv(\n",
        "    os.path.join(data_dir, \"skill_builder_data.csv\"),\n",
        "    sep=\",\",\n",
        ")\n",
        "\n",
        "print(data.head(10))\n",
        "print(data.user_id.nunique())\n",
        "print(data.shape)\n",
        "print(data.correct.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27aa7030",
      "metadata": {},
      "source": [
        "# Preprocessing\n",
        "\n",
        "Since datasets can differ in how they are formatted. In the **data_loaders** folder there are preprocessing scripts. These are called when you run the model (no need to call them separately). We can run *train.py* with the option *--only_preprocess* to run only the preprocessing, to avoid a long wait. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "d098e90d",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(\"DeepModels/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "6c129767",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train.py --model_name=dkvmn --dataset_name=ASSIST2009 --only_preprocess "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaff23eb",
      "metadata": {},
      "source": [
        "Briefly, the preprocessing implements the following steps:\n",
        "\n",
        "- Sort responses by log_id (time) for each student.\n",
        "- Remove observations if correct is 1 or 0.\n",
        "- Reindex students and items.\n",
        "- Split sequences of KCs and responses so that they have uniform length (*seq_len*).\n",
        "- Split dataset in train and test (e.g., 90%/10%).\n",
        "\n",
        "The preprocessing creates the following files that are used to train the model:\n",
        "\n",
        "*u_list.pkl*: List of student ids.\n",
        "*u2idx.pkl*: Dict with mapping of student ids to indices.\n",
        "\n",
        "*q_list.pkl*: List of knowledge component (KC) ids.\n",
        "*q2idx.pkl*: Dict with mapping of KC ids to indices.\n",
        "\n",
        "*r_seqs.pkl*: List of sequences of responses. Each element is an array that corresponds to the sequence of responses of one student, ordered by timestamp (log_id).\n",
        "*q_seqs.pkl*: List of sequences of KCs. Each element is an array that corresponds to the sequence of KCs of one student, ordered by timestamp (log_id). It matches r_seqs.\n",
        "\n",
        "*train_indices.pkl*, *test_indices.pkl*: training/test indices\n",
        "\n",
        "Inspect these files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "5313dc76",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   14 21825 51950 52613 53167]\n",
            "['Absolute Value' 'Addition Whole Numbers'\n",
            " 'Addition and Subtraction Fractions' 'Addition and Subtraction Integers'\n",
            " 'Addition and Subtraction Positive Decimals']\n",
            "[array([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 45, 70, 70, 45, 70,\n",
            "       45, 45]), array([90, 90, 90, 90, 90, 90, 90, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48]), array([30, 30, 30, 30, 29, 29]), array([ 4, 33, 51, 25, 25, 25]), array([ 4,  4,  4,  4,  3,  3, 48, 48, 21, 77, 48, 47,  3,  2,  2,  2,  2,\n",
            "        2,  1,  1,  1,  2,  4, 48,  2,  2,  4, 55, 56, 48, 52, 52, 52, 84,\n",
            "       89, 89, 89, 89, 50, 16, 90, 90, 66, 66, 90, 90, 90, 90, 90, 90, 22,\n",
            "       87,  1,  1, 87, 56, 54, 67, 55, 30, 30, 30, 30, 30, 30, 30, 30, 24,\n",
            "       20, 20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 90, 90, 90, 16,  9,  9,\n",
            "        9,  9,  9,  1,  1,  3,  3,  3,  3,  4,  2,  2,  1,  1,  1,  1,  1,\n",
            "       56, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 92, 92, 92, 92, 92, 92,\n",
            "       60, 60, 60, 60, 60, 49, 49, 49, 49, 27, 27, 27, 27, 48, 48, 48, 48,\n",
            "       48, 33, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 48, 48,  3, 43, 43,\n",
            "       43, 43, 43, 43, 43, 43, 56, 56, 56, 54, 54, 54, 54, 54, 54, 55, 55,\n",
            "       55, 55, 55, 55, 55, 55, 55, 53, 52, 52, 52, 52, 52, 52, 53, 53, 53,\n",
            "       53, 53, 60, 64, 77,  3, 21, 33, 50, 85, 85, 85, 85, 85, 85, 67, 67,\n",
            "       67, 67, 67, 67, 34, 26, 32, 32, 32, 80, 80, 80, 80, 80, 68, 46, 53,\n",
            "       53, 53, 53, 90, 90, 90, 90, 90, 45, 44, 30, 30, 30, 30, 30, 30, 30,\n",
            "       30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 79,\n",
            "       79, 79, 79, 79, 58, 16, 16, 16, 16, 16, 16, 16, 67])]\n",
            "[array([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0]), array([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([0, 1, 1, 1, 1, 1]), array([0, 1, 0, 1, 1, 1]), array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
            "       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
            "       1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
            "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "       1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
            "       0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
            "       1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
            "       1, 1, 1, 1])]\n",
            "5185\n",
            "577\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"./datasets/ASSIST2009\"\n",
        "pkl_files = [f for f in os.listdir(data_dir) if f.endswith(\".pkl\")]\n",
        "\n",
        "\n",
        "data_dict = {}\n",
        "\n",
        "\n",
        "for file in pkl_files:\n",
        "\n",
        "    with open(os.path.join(data_dir, file), \"rb\") as f:\n",
        "        data_dict[os.path.splitext(file)[0]] = pickle.load(f)\n",
        "\n",
        "\n",
        "print(data_dict[\"u_list\"][:5])\n",
        "print(data_dict[\"q_list\"][:5])\n",
        "\n",
        "\n",
        "print(data_dict[\"q_seqs\"][:5])\n",
        "\n",
        "\n",
        "print(data_dict[\"r_seqs\"][:5])\n",
        "\n",
        "print(len(data_dict[\"train_indices\"]))\n",
        "print(len(data_dict[\"test_indices\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b9095f",
      "metadata": {},
      "source": [
        "*q_seqs*, *r_seqs* are finally converted in datasets which consists of pairs *(q, r)*, where *q* and *r* are 1-d arrays. *q* is a sequence of KC indices and *r* a sequence of responses (0 or 1). The sequences have length *seq_len* and are padded at the end with -1. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23898c33",
      "metadata": {},
      "source": [
        "# Configuration\n",
        "\n",
        "The configuration file config.json allows to specify parameters for the training and for the different models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "e507e102",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"train_config\": {\n",
            "        \"batch_size\": 256,\n",
            "        \"num_epochs\": 100,\n",
            "        \"train_ratio\": 0.9,\n",
            "        \"learning_rate\": 0.001,\n",
            "        \"optimizer\": \"adam\",\n",
            "        \"seq_len\": 100\n",
            "    },\n",
            "    \"dkt\": {\n",
            "        \"emb_size\": 100,\n",
            "        \"hidden_size\": 100\n",
            "    },\n",
            "    \"dkvmn\": {\n",
            "        \"dim_s\": 50,\n",
            "        \"size_m\": 20\n",
            "    },\n",
            "    \"sakt\": {\n",
            "        \"n\": 100,\n",
            "        \"d\": 100,\n",
            "        \"num_attn_heads\": 5,\n",
            "        \"dropout\": 0.2\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "with open(\"config.json\", \"r\") as f:\n",
        "    print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6038d1",
      "metadata": {},
      "source": [
        "# Fitting the model\n",
        "\n",
        "Now you can run the model. After running it, results (loss and aucs) for the test set can be found in the folder *ckpts*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c49e2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_name can be dkt, dkvmn or sakt\n",
        "!python train.py --model_name=dkvmn --dataset_name=ASSIST2009"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0aa364c",
      "metadata": {},
      "source": [
        "Since this takes a while to run, you have the precomputed results for three models (dkt, dkvmn, sakt) in the folder *ckpts_precomputed/[model_name]/[dataset_name]*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce72a67f",
      "metadata": {},
      "outputs": [],
      "source": [
        "results_dict = {}\n",
        "combined_df = None\n",
        "for model_name in [\"dkt\", \"dkvmn\", \"sakt\"]:\n",
        "    ckpt_dir = f\"ckpts_precomputed/{model_name}/ASSIST2009\"\n",
        "    pkl_files = [f for f in os.listdir(ckpt_dir) if f.endswith(\".pkl\")]\n",
        "\n",
        "    if len(pkl_files) == 0:\n",
        "        continue\n",
        "\n",
        "    results_dict[model_name] = {}\n",
        "\n",
        "    for file in pkl_files:\n",
        "        with open(os.path.join(ckpt_dir, file), \"rb\") as f:\n",
        "            results_dict[model_name][os.path.splitext(file)[0]] = pickle.load(f)\n",
        "\n",
        "    # Create DataFrames for plotting\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"Epoch\": range(len(results_dict[model_name][\"aucs\"])),\n",
        "            \"AUC\": results_dict[model_name][\"aucs\"],\n",
        "            \"Loss\": results_dict[model_name][\"loss_means\"],\n",
        "            \"Model\": model_name,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if combined_df is None:\n",
        "        combined_df = df\n",
        "    else:\n",
        "        combined_df = pd.concat([combined_df, df])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "# Plot Loss\n",
        "sns.lineplot(ax=axes[0], data=combined_df, x=\"Epoch\", y=\"Loss\", hue=\"Model\")\n",
        "axes[0].set_title(\"Loss\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot AUC\n",
        "sns.lineplot(ax=axes[1], data=combined_df, x=\"Epoch\", y=\"AUC\", hue=\"Model\")\n",
        "axes[1].set_title(\"AUC\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"AUC\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AMLDAIEd",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
